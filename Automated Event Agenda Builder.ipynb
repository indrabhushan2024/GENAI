{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain[all]\n",
        "! pip install langchain_community[all]\n",
        "! pip install unstructured[all]\n",
        "! pip install openai[all]\n",
        "! pip install python-docx\n",
        "! pip install pdfminer.six\n",
        "! pip install pillow_heif\n",
        "! pip install nltk\n",
        "! pip install spacy\n",
        "! pip install unstructured_inference\n",
        "! pip install -U langchain-unstructured\n",
        "! pip install unstructured\n",
        "! pip install unstructured[local-inference]\n",
        "! pip install pdf2image\n",
        "! pip install pdfminer.six\n",
        "! apt-get install poppler-utils\n",
        "! apt-get install tesseract-ocr\n",
        "!pip uninstall -y nltk\n",
        "!pip install nltk\n",
        "! openai migrate\n",
        "! pip install openai==0.28\n",
        "# ! pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt4zbCFjkJG6",
        "outputId": "efacf6ae-39f1-4e5f-dc5e-5d08df47ea9e"
      },
      "id": "Qt4zbCFjkJG6",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (5,602 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123625 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pdf2image\n",
        "! pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHAW4CC_r_MS",
        "outputId": "2159ca16-0b1c-4383-ec0f-4bf8fa48a968"
      },
      "id": "wHAW4CC_r_MS",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (10.4.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20231228)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ce3c30f",
      "metadata": {
        "tags": [],
        "id": "6ce3c30f",
        "outputId": "b90596a3-f365-4e69-fbc8-40aa9187e555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain_unstructured import UnstructuredLoader\n",
        "import openai\n",
        "import os\n",
        "from unstructured.partition.docx import partition_docx\n",
        "from unstructured.partition.doc import partition_doc\n",
        "from unstructured.partition.xlsx import partition_xlsx\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.partition.email import partition_email\n",
        "from unstructured.cleaners.core import clean\n",
        "import nltk\n",
        "import spacy\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('state_union')\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_version = \"2023-03-15-preview\"\n",
        "openai.api_base = 'https://micodefest-oai.openai.azure.com/'  # Your Azure OpenAI resource's endpoint value .\n",
        "openai.api_key = '3a039447ea0141e08f19fba1024c427d'\n",
        "\n",
        "def get_completion(text, prompt, model=\"GPT3516KTEST\"):\n",
        "    messages = [{ \"role\": \"system\", \"content\": text}, {\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "    resp = response.choices[0].message[\"content\"]\n",
        "    return resp\n",
        "\n",
        "class Agenda_Builder:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.file_content=\"\"\n",
        "\n",
        "    #Extracting contents from pdf\n",
        "    def extract_text(self,filename):\n",
        "        try:\n",
        "            loader=UnstructuredFileLoader(filename)\n",
        "            docs=loader.load()\n",
        "            return docs\n",
        "        except FileNotFoundError:\n",
        "            print(\"The file does not exists\")\n",
        "\n",
        "\n",
        "    #Cleaning the extracted contents\n",
        "    def clean_text(self,filename):\n",
        "        concatenated_RFP_text_by_category = {}\n",
        "        file_extension = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "        if file_extension == '.docx':\n",
        "            elements = partition_docx(filename=filename)\n",
        "        elif file_extension == '.doc':\n",
        "            elements = partition_doc(filename=filename)\n",
        "        elif file_extension == '.xlsx':\n",
        "            elements = partition_xlsx(filename=filename)\n",
        "        elif file_extension == '.pdf':\n",
        "            elements = partition_pdf(filename)\n",
        "        elif file_extension == '.eml':\n",
        "            elements = partition_email(filename)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "        if ((file_extension == '.docx') or (file_extension == '.doc')):\n",
        "\n",
        "            for elem in elements:\n",
        "                category = elem.category\n",
        "                RFP_text = elem.text\n",
        "                cleaned_text = clean(RFP_text, extra_whitespace=True, dashes=True)\n",
        "\n",
        "                if category in concatenated_RFP_text_by_category:\n",
        "                    concatenated_RFP_text_by_category[category].append(cleaned_text)\n",
        "                else:\n",
        "                    concatenated_RFP_text_by_category[category] = [cleaned_text]\n",
        "                # print(concatenated_RFP_text_by_category)\n",
        "            for category, RFP_text in concatenated_RFP_text_by_category.items():\n",
        "                if category in [\"Title\", \"UncategorizedText\", \"Table\"]:\n",
        "                    concatenated_RFP_text = ' '.join(RFP_text)\n",
        "                    print(f'Category {category}: {concatenated_RFP_text}')\n",
        "                    # doc=nlp(concatenated_RFP_text)\n",
        "                    # sentences=list(doc.sents)\n",
        "                    # print(\"--------------Sentences-----------------\")\n",
        "                    # print(sentences)\n",
        "                    # print(\"-----------------------------------------\")\n",
        "                    # for token in doc:\n",
        "                        #print(token.text)\n",
        "                        #ents=[(e.text,e.start_char,e.end_char,e.label_) for e in doc.ents]\n",
        "                             #if e.label_ in [\"EVENT\", \"DATE\", \"TIME\"]]\n",
        "                        #print(ents)\n",
        "                        #displayc.render(doc,style='ent',jupyter=True)\n",
        "\n",
        "\n",
        "                    print(\"\\n\")\n",
        "\n",
        "        elif file_extension == '.xlsx':\n",
        "            for elem in elements:\n",
        "                category = elem.category\n",
        "                RFP_text = elem.text\n",
        "                cleaned_text = clean(RFP_text, extra_whitespace=True, dashes=True)\n",
        "\n",
        "                if category in concatenated_RFP_text_by_category:\n",
        "                    concatenated_RFP_text_by_category[category].append(cleaned_text)\n",
        "                else:\n",
        "                    concatenated_RFP_text_by_category[category] = [cleaned_text]\n",
        "    # print(concatenated_RFP_text_by_category)\n",
        "                for category, RFP_text in concatenated_RFP_text_by_category.items():\n",
        "                    if category in [\"Table\"]:\n",
        "                        concatenated_RFP_text = ' '.join(RFP_text)\n",
        "                        # doc=nlp(concatenated_RFP_text)\n",
        "                        # sentences=list(doc.sents)\n",
        "                        # print(sentences)\n",
        "                        # for token in doc:\n",
        "                        #     print(token.text)\n",
        "                        #     ents=[(e.text,e.start_char,e.end_char,e.label_) for e in doc.ents]\n",
        "                        #     #if e.label_ in [\"EVENT\", \"DATE\", \"TIME\"]]\n",
        "                        #     print(ents)\n",
        "\n",
        "        elif file_extension == '.pdf':\n",
        "            for elem in elements:\n",
        "                category = elem.category\n",
        "                RFP_text = elem.text\n",
        "\n",
        "                cleaned_text = clean(RFP_text, extra_whitespace=True, dashes=True)\n",
        "\n",
        "                if category in concatenated_RFP_text_by_category:\n",
        "                    concatenated_RFP_text_by_category[category].append(cleaned_text)\n",
        "                else:\n",
        "                    concatenated_RFP_text_by_category[category] = [cleaned_text]\n",
        "            print(concatenated_RFP_text_by_category)\n",
        "            for category, RFP_text in concatenated_RFP_text_by_category.items():\n",
        "                if category in [\"NarrativeText\", \"UncategorizedText\", \"Title\"]:\n",
        "                    concatenated_RFP_text = ' '.join(RFP_text)\n",
        "                #     doc=nlp(concatenated_RFP_text)\n",
        "                #     sentences=list(doc.sents)\n",
        "                #     print(sentences)\n",
        "                #     for token in doc:\n",
        "                #         #print(token.text)\n",
        "                #         ents=[(e.text,e.start_char,e.end_char,e.label_) for e in doc.ents]\n",
        "                        #print(ents)\n",
        "        return concatenated_RFP_text_by_category\n",
        "\n",
        "    def ai_prompt(self,cl_text):\n",
        "\n",
        "        print(\"-------------------------------------------------------------------------------\")\n",
        "\n",
        "        prompt = \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "                    Context and Role:\n",
        "                    - You are an expert backend event scheduler working at a prestigious hotel.\n",
        "                    - Your primary responsibility is to meticulously extract event agenda details from unstructured text documents (RFPs) presented in tabular form.\n",
        "                    - Your commitment is to provide accurate and complete information in a structured JSON format.\n",
        "\n",
        "\n",
        "\n",
        "                    Output Format:\n",
        "                    - Generate JSON objects with the following keys for each event: date, day, startTime, endTime, functionType, setupStyle, peopleCount, comments.\n",
        "                    - Ensure that each JSON object contains all of the specified keys.\n",
        "                    - Verify that all values are correctly populated.\n",
        "\n",
        "\n",
        "\n",
        "                    Extraction Requirements:\n",
        "                    - For every row in the RFP that provides event agenda details, it is imperative to generate a corresponding, accurately formatted row in the JSON output.\n",
        "                    - Pay meticulous attention to detail, ensuring that no rows from the RFP are missed during extraction.\n",
        "                    - Never combine multiple rows from the RFP into a single row in the JSON output, and do not permit the division of a single RFP row into multiple JSON rows.\n",
        "                    - Achieve comprehensive coverage by including all events mentioned in the RFP, regardless of the total number of rows.\n",
        "\n",
        "\n",
        "\n",
        "                    Specific Date Range Instructions:\n",
        "                    - If the RFP provides a specific date range (e.g., \"Sep 1 2023 - Sep 7 2023\"), explicitly create entries for each day within the range. Ensure no days are omitted.\n",
        "                    - Always format dates as MM/DD/YYYY.\n",
        "\n",
        "\n",
        "\n",
        "                    Output Structure:\n",
        "                    - Maintain the JSON output structure consistently, which should always have the following format:\n",
        "                    {\n",
        "                        \"events\": [\n",
        "                            {\n",
        "                                \"date\": \"MM/DD/YYYY\",\n",
        "                                \"day\": \"Day of the week\",\n",
        "                                \"startTime\": \"Start time\",\n",
        "                                \"endTime\": \"End time\",\n",
        "                                \"functionType\": \"Event type\",\n",
        "                                \"setupStyle\": \"Setup style\",\n",
        "                                \"peopleCount\": \"Number of people\",\n",
        "                                \"comments\": \"Additional comments\"\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "\n",
        "\n",
        "\n",
        "                    Comprehensive Coverage Assurance:\n",
        "                    - Prioritize thoroughness and diligence to ensure no event agenda details are missed. Review the generated JSON output meticulously to confirm that all rows from the RFP are accurately captured.\n",
        "\n",
        "\n",
        "                    Your role demands the utmost precision. Your task is to meticulously extract event agenda details from tabular\n",
        "                    RFP text while strictly adhering to the specified guidelines. The resulting JSON output should consistently\n",
        "                    adhere to the defined structure and content requirements, and all responses should be deterministic with a temperature of 0.\n",
        "          \"\"\"\n",
        "        response = get_completion(cl_text,prompt)\n",
        "\n",
        "        return response\n",
        "        #save_file=open(\"interim_file.json\",\"w\")\n",
        "        #json.dump(response,save_file,indent=6)\n",
        "\n",
        "    def post_processing(self,res):\n",
        "        # List of synonyms\n",
        "        function_types = [\n",
        "            'Board Meeting', 'Box Lunch', 'Breakfast', 'Breakfast Buffet', 'Breakout', 'Brunch', 'Ceremony', 'Changing Room',\n",
        "            'Coat Check', 'Cocktail Reception', 'Coffee Break', 'Continental Breakfast', 'Continuous Break', 'Dance', 'Dinner',\n",
        "            'Dinner Buffet', 'Exhibits', 'General Session', 'Holding Room', 'Hospitality Room', 'In-house Meeting', 'Interview',\n",
        "            'Lunch', 'Lunch Buffet', 'Meal on Own', 'Meeting', 'Menu Tasting', 'No Agenda Hold', 'Off Site', 'Office', 'Reception',\n",
        "            'Recreation', 'Registration', 'Rehearsal', 'Room Ready', 'Set Up', 'Speaker Room', 'Special',\n",
        "            'Storage', 'Teardown',\n",
        "            'Trade Show'\n",
        "        ]\n",
        "\n",
        "        setup_styles = [\n",
        "            \"Chevron Schoolroom\", \"Chevron Theatre\", \"Cocktail Rounds\", \"Conference\",\n",
        "            \"Conference 2 per 6\", \"Conference 3 per 8\",\"Crescent Rounds\", \"Exhibits\",\n",
        "            \"Hollow Square\", \"Hollow Square 2 per 6\", \"Hollow Square 3 per 8\", \"Lounge\", \"Off Site\",\n",
        "            \"Oval Conference\", \"Registration\", \"Rounds of 10\", \"Rounds of 12\", \"Rounds of 6\",\n",
        "            \"Rounds of 8\", \"Schoolroom\", \"Schoolroom 2 per 6\",\"Schoolroom 3 per 8\", \"Special\",\n",
        "            \"Storage\", \"Theatre\", \"U-Shape\", \"U-Shape 2 per 6\", \"U-Shape 3 per 8\"\n",
        "        ]\n",
        "\n",
        "        # f=open('interim_file.json')\n",
        "        # data=json.load(f)\n",
        "        #\n",
        "        # Create a prompt to find the most identical synonym\n",
        "\n",
        "        prompt = \"\"\"\n",
        "\n",
        "        - For each record in input json, perform ALL of the below\n",
        "\n",
        "            - Scrutinize functionType and setupStyle for the presence of \"24-hour hold\" or \"registration counters,\" and ensure that this information is consistently and prominently included in the comments section.\n",
        "\n",
        "            - If the functionType is specified as \"24-hour hold,\" unconditionally set the endTime to 6:00 PM.\n",
        "\n",
        "            - If funtionType is not in the given function_types list replace funtionType with the closest value from function_types list.\n",
        "\n",
        "            - If setupStyle is not in the given setup_styles list replace setupStyle with the closest value from setup_styles list.\n",
        "\n",
        "\n",
        "            - If there are identifical rows (all key values match) in json, remove duplicates\n",
        "\n",
        "            Output Structure:\n",
        "            - Maintain the JSON output structure consistently, which should always have the following format:\n",
        "            {\n",
        "                \"events\": [\n",
        "                    {\n",
        "                        \"date\": \"MM/DD/YYYY\",\n",
        "                        \"day\": \"Day of the week\",\n",
        "                        \"startTime\": \"Start time\",\n",
        "                        \"endTime\": \"End time\",\n",
        "                        \"functionType\": \"Event type\",\n",
        "                        \"setupStyle\": \"Setup style\",\n",
        "                        \"peopleCount\": \"Number of people\",\n",
        "                        \"comments\": \"Additional comments\"\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        resp = get_completion(f\"\"\"{res}\"\"\",prompt)\n",
        "\n",
        "        # Print the updated response\n",
        "        print(\"Updated response:\", resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a79bf27e",
      "metadata": {
        "tags": [],
        "id": "a79bf27e"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # filename='Tractor_Supply.xlsx'\n",
        "    filename='/content/sample_data/Los Angeles Hotel 1183_2024 space addendum_dl.pdf'\n",
        "\n",
        "    AB=Agenda_Builder()\n",
        "\n",
        "    pdf_text=AB.extract_text(filename)\n",
        "\n",
        "    cln_text=AB.clean_text(filename)\n",
        "\n",
        "    response=AB.ai_prompt(str(cln_text))\n",
        "\n",
        "  #  print(\"------------- Extracted Text------------------------------\")\n",
        "    #print(pdf_text)\n",
        "    #print('\\n')\n",
        "    print(\"-------------Cleaned Text---------------------------------\")\n",
        "    print(cln_text)\n",
        "    print('\\n')\n",
        "    print(\"-------------Prompt Output--------------------------------\")\n",
        "    print(response)\n",
        "    print('\\n')\n",
        "    print(\"-------------Post processing------------------------------\")\n",
        "    AB.post_processing(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validating the NLTK Libriry**"
      ],
      "metadata": {
        "id": "7HT9SgW2xisX"
      },
      "id": "7HT9SgW2xisX"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'punkt' tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(nltk.data.path)\n",
        "\n",
        "# Set NLTK data path\n",
        "nltk.data.path.append('/root/nltk_data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6dn6LyMv2rP",
        "outputId": "46649fce-60f6-4d3a-db2d-dfab0e9255d9"
      },
      "id": "c6dn6LyMv2rP",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validating the NLTK Libriry - Word Tokenizer**"
      ],
      "metadata": {
        "id": "l2YvCLolxrPA"
      },
      "id": "l2YvCLolxrPA"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'punkt' tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Test tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello, how are you doing today?\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofCXPCNvwndp",
        "outputId": "88d26e43-75d6-4ad6-ec69-11ce2f64ee83"
      },
      "id": "ofCXPCNvwndp",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U langchain-unstructured\n",
        "# ! pip uninstall langchain-unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B56w4Xi0AmE",
        "outputId": "00e82ed1-33a7-4111-ab84-186b3f52fe3c"
      },
      "id": "7B56w4Xi0AmE",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-unstructured\n",
            "  Using cached langchain_unstructured-0.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.23 in /usr/local/lib/python3.10/dist-packages (from langchain-unstructured) (0.2.34)\n",
            "Requirement already satisfied: unstructured-client<0.25.0,>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from langchain-unstructured) (0.24.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (0.1.104)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (4.12.2)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (2024.7.4)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (3.3.2)\n",
            "Requirement already satisfied: dataclasses-json>=0.6.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (0.6.7)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (0.27.0)\n",
            "Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (3.7)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (1.0.6)\n",
            "Requirement already satisfied: marshmallow>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (3.22.0)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (1.6.0)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (4.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (1.0.0)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (1.16.0)\n",
            "Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (0.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (2.0.7)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (4.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.23->langchain-unstructured) (2.20.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client<0.25.0,>=0.24.1->langchain-unstructured) (1.2.2)\n",
            "Using cached langchain_unstructured-0.1.2-py3-none-any.whl (6.7 kB)\n",
            "Installing collected packages: langchain-unstructured\n",
            "Successfully installed langchain-unstructured-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e2a7118f",
      "metadata": {
        "tags": [],
        "id": "e2a7118f",
        "outputId": "2757fc8c-ab66-4941-c9ce-3cb3740e2d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Title': ['Contract Addendum Program Number: Program Name: City: Number of Nights:', 'Original Request: Space Request as o', 'Hotel:', 'Configuration Request:', 'No.', 'Day at Site (Including arrival & departure)', 'Meals', 'D', 'B', 'B', 'D', 'B', 'D', 'B', 'D', 'B', 'Accepted by Hotel Representative Signature:', \"1183 Art Collector's and Their Collections\", 'Los Angeles, CA 5', 'Operational Road Scholar Contact: Erinn Green', 'Phone: Email:', 'singles/doubles interchangeable', 'Hotel Arrival', 'Hotel Departure', 'Meeting Room Needs', 'Start and End Time', 'Additional Staff Room Notes **if not for the full duration', 'Buffet or Plated in Hotel Restaurant with coffee/tea', '7AM 9AM', 'classroom style set up, microphone package, podium/screen, access to power for BYO projector/laptop', '7AM 9AM', 'classroom style set up, microphone package, podium/screen, access to power for BYO projector/laptop', 'Buffet or Plated in Hotel Restaurant with coffee/tea', '7AM 9AM', 'classroom style set up, microphone package, podium/screen, access to power for BYO projector/laptop', 'Buffet or Plated in Hotel Restaurant with coffee/tea', '7AM 9AM', 'classroom style set up, microphone package, podium/screen, access to power for BYO projector/laptop', 'Buffet or Plated in Hotel Restaurant with coffee/tea', '7AM 9AM', 'classroom style set up, microphone package, podium/screen, access to power for BYO projector/laptop', 'Date:'], 'UncategorizedText': ['7/14/2022 7/19/2022', '1 2 3 4 5 6 7 8 9 10 11 12 13 14 15', 'Program Departure Date 14 Jan 24 28 Jan 24 11 Feb 24 03 Mar 24 24 Mar 24 21 Apr 24 19 May 24 23 Jun 24 01 Sep 24 22 Sep 24 20 Oct 24', '1', '1', '2', '2', '3', '3', '3', '4', '4', '4', '5', '5', '5', '6', '6', '503 549 3010', 'Sunday, January 14, 2024 Sunday, January 28, 2024 Sunday, February 11, 2024 Sunday, March 3, 2024 Sunday, March 24, 2024 Sunday, April 21, 2024 Sunday, May 19, 2024 Sunday, June 23, 2024 Sunday, September 1, 2024 Sunday, September 22, 2024 Sunday, October 20, 2024', 'Friday, January 19, 2024 Friday, February 2, 2024 Friday, February 16, 2024 Friday, March 8, 2024 Friday, March 29, 2024 Friday, April 26, 2024 Friday, May 24, 2024 Friday, June 28, 2024 Friday, September 6, 2024 Friday, September 27, 2024 Friday, October 25, 2024', 'Room Block* 23 23 23 23 23 23 23 23 16 16 23', 'GL Rooms 1 1 1 1 1 1 1 1 1 1 1', 'Addtl. Staff Rooms** 0 0 0 0 0 0 0 0 0 0 0', 'Total # of Rooms 24 24 24 24 24 24 24 24 17 17 24 0 0 0 0', 'Notes for Meals, Meeting Room set up, AV requests, Porterage needed, etc,', '2:30PM 8:00PM', '6PM 8:30PM', '9 11:30AM, 7 9:30PM', '9 11:30AM, 7 9:30PM', '6PM 8:30PM', '9 11:30AM, 7 9:30PM', '6PM 8:30PM', '9 11:30AM, 7 9:30PM', '6PM 8:30PM', '8 11:30AM'], 'NarrativeText': ['Wi fi is complimentary in meeting room Hotel Representative: Phone: Email:', 'No porterage needed. Welcome/Orientation: skirted 6 ft welcome table with two chairs in a visable location (lobby); classroom or theater set up for orientation class, podium, mic', 'No porterage needed.'], 'EmailAddress': ['erinn.green@roadscholar.org']}\n",
            "-------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PermissionError",
          "evalue": "Access denied due to Virtual Network/Firewall rules.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-d0d65c8e2b50>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcln_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcln_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m#  print(\"------------- Extracted Text------------------------------\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-6f49f7b931fb>\u001b[0m in \u001b[0;36mai_prompt\u001b[0;34m(self, cl_text)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0madhere\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefined\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0mrequirements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m \u001b[0mresponses\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m           \"\"\"\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-6f49f7b931fb>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(text, prompt, model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GPT3516KTEST\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mPermissionError\u001b[0m: Access denied due to Virtual Network/Firewall rules."
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify API Keys and Permissions - Access denied due to Virtual Network/Firewall rules**"
      ],
      "metadata": {
        "id": "urX8xu8r1jB-"
      },
      "id": "urX8xu8r1jB-"
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set your API key\n",
        "openai.api_key = '3a039447ea0141e08f19fba1024c427d'\n",
        "\n",
        "# Example request to the OpenAI API\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=\"Translate the following English text to French: 'Hello, how are you?'\",\n",
        "    max_tokens=60\n",
        ")\n",
        "\n",
        "print(response.choices[0].text.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ytkHtp411KTR",
        "outputId": "0ff65863-6851-4b9f-b06e-76aa95e12405"
      },
      "id": "ytkHtp411KTR",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PermissionError",
          "evalue": "Access denied due to Virtual Network/Firewall rules.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2b4a575b55a7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Example request to the OpenAI API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m response = openai.Completion.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Translate the following English text to French: 'Hello, how are you?'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mPermissionError\u001b[0m: Access denied due to Virtual Network/Firewall rules."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      }
    ],
    "instance_type": "ml.m5.large",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}